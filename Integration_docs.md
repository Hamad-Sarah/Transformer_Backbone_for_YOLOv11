Okay, I have the necessary information from the `cvnets` MobileViTv2 implementation (`mobilevit_v2.py` and its config).

Based on these files, here is the proposed wrapper class `MobileViTv2Backbone`. This code should be placed in a new file within your Ultralytics project.

```python
import torch
import torch.nn as nn
import argparse
from typing import List, Tuple

# --- Attempt to import MobileViTv2 from cvnets ---
# Ensure the cvnets library is installed and accessible in your environment
try:
    from cvnets.models.classification.mobilevit_v2 import MobileViTv2
    # We also need the config function to determine output channels dynamically
    from cvnets.models.classification.config.mobilevit_v2 import get_configuration
    CVNETS_AVAILABLE = True
except ImportError:
    print("Warning: cvnets library not found or MobileViTv2 could not be imported. "
          "MobileViTv2Backbone module will not be functional.")
    MobileViTv2 = None # Define as None to allow conditional checks
    get_configuration = None
    CVNETS_AVAILABLE = False
# --- End Import ---

class MobileViTv2Backbone(nn.Module):
    """
    Wrapper module for using MobileViTv2 (from cvnets) as a backbone feature extractor
    in the Ultralytics framework.
    """
    def __init__(self, width_multiplier: float = 1.0, return_indices: Tuple[int] = (2, 3, 4), pretrained: bool = False):
        """
        Initializes the MobileViTv2Backbone wrapper.

        Args:
            width_multiplier (float): Width multiplier for MobileViTv2 config.
            return_indices (Tuple[int]): Indices of the feature maps to return from MobileViTv2's output list.
                                         Defaults to (2, 3, 4) corresponding roughly to P3, P4, P5 stages.
            pretrained (bool): If True, attempts to load pretrained weights (Not implemented in this basic wrapper).
        """
        super().__init__()

        if not CVNETS_AVAILABLE:
            raise ImportError("cvnets library is required to use MobileViTv2Backbone, but it's not installed or accessible.")

        self.return_indices = return_indices

        # --- Configure MobileViTv2 using opts ---
        # Create a namespace to mimic the argument parsing used by cvnets
        opts = argparse.Namespace()

        # Set essential MobileViTv2 configuration options
        setattr(opts, "model.classification.mitv2.width_multiplier", width_multiplier)
        setattr(opts, "model.classification.n_classes", 1000) # Placeholder, not used for feature extraction
        setattr(opts, "model.layer.global_pool", "mean") # Placeholder, not used for feature extraction
        setattr(opts, "model.classification.mitv2.dropout", 0.0) # Typically disable dropout for backbone features
        setattr(opts, "model.classification.mitv2.attn_dropout", 0.0)
        setattr(opts, "model.classification.mitv2.ffn_dropout", 0.0)
        setattr(opts, "model.classification.mitv2.attn_norm_layer", "layer_norm_2d")
        setattr(opts, "common.enable_coreml_compatible_module", False)
        setattr(opts, "model.classification.enable_layer_wise_lr_decay", False) # Not relevant for backbone use
        setattr(opts, "model.classification.layer_wise_lr_decay_rate", 0.9) # Not relevant
        setattr(opts, "model.normalization.name", "batch_norm") # Standard norm
        setattr(opts, "model.normalization.momentum", 0.1)
        setattr(opts, "model.normalization.groups", 1)
        setattr(opts, "model.activation.name", "swish") # Common activation
        setattr(opts, "model.activation.inplace", False)
        setattr(opts, "model.activation.neg_slope", 0.1) # For leaky relu if used, but swish is default

        # Add any other options required by the specific MobileViTv2 version if needed
        # --- End opts setup ---

        # Instantiate the core MobileViTv2 model
        # Note: MobileViTv2 internally removes the final classifier if num_classes=0,
        # but we are modifying its forward pass anyway.
        self.mobilevit = MobileViTv2(opts=opts)

        # --- Determine output channels ---
        # We need the configuration to know the channel dimensions at each stage
        mobilevit_config = get_configuration(opts=opts)
        # Store channel dimensions of all layers generated by MobileViTv2
        # Order: conv_1, layer_1, layer_2, layer_3, layer_4, layer_5, conv_1x1_exp
        all_layer_channels = [
            mobilevit_config["layer0"]["out_channels"],
            mobilevit_config["layer1"]["out_channels"],
            mobilevit_config["layer2"]["out_channels"],
            mobilevit_config["layer3"]["out_channels"],
            mobilevit_config["layer4"]["out_channels"],
            mobilevit_config["layer5"]["out_channels"],
            mobilevit_config["layer5"]["out_channels"], # conv_1x1_exp is Identity, keeps channels from layer5
        ]
        # Select the channels corresponding to the features we will return
        self._out_channels = [all_layer_channels[i] for i in self.return_indices]

        # --- Pretrained weights ---
        if pretrained:
            # TODO: Implement loading pretrained weights if available/needed
            # This would typically involve finding a weight file and loading
            # the state_dict into self.mobilevit, potentially mapping keys.
            print("Warning: Pretrained weights loading not implemented in this wrapper.")
            pass

        # Remove the original classifier explicitly if it exists (MobileViTv2 might already handle this)
        # hasattr check is safer
        if hasattr(self.mobilevit, 'classifier'):
            del self.mobilevit.classifier
            self.mobilevit.classifier = nn.Identity() # Replace with identity


    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        """
        Runs the forward pass through MobileViTv2 and returns selected feature maps.

        Args:
            x (torch.Tensor): Input tensor (typically from the layer before the backbone in YOLO).

        Returns:
            List[torch.Tensor]: A list of the selected feature map tensors.
        """
        # MobileViTv2's forward method (as modified in the provided file)
        # already returns a list of all intermediate feature maps.
        all_feature_maps = self.mobilevit(x)

        # Select only the feature maps specified by self.return_indices
        selected_features = [all_feature_maps[i] for i in self.return_indices]

        return selected_features

    @property
    def out_channels(self) -> List[int]:
        """
        Returns the number of output channels for each feature map returned by the forward pass.
        Needed by the Ultralytics model parser.
        """
        return self._out_channels

# Example Usage (for testing within this file)
if __name__ == '__main__':
    if CVNETS_AVAILABLE:
        # Test with default settings (width_multiplier=1.0)
        print("Testing MobileViTv2Backbone (width_multiplier=1.0)...")
        backbone = MobileViTv2Backbone(width_multiplier=1.0)
        print(f"Output channels: {backbone.out_channels}")

        # Expecting input similar to P2/4 output, e.g., [B, 128, H/4, W/4]
        # For a 256x256 input to YOLO, P2/4 might be 64x64. Let's use that.
        # The channel count needs to match the layer feeding into this backbone in YOLO.
        # Let's assume the layer before this outputs 128 channels (like YOLOv11n layer 1)
        dummy_input = torch.randn(1, 128, 64, 64)
        print(f"Input shape: {dummy_input.shape}")

        try:
            output_features = backbone(dummy_input)
            print(f"Obtained {len(output_features)} feature maps:")
            for i, fm in enumerate(output_features):
                print(f"  Feature map {i} (from index {backbone.return_indices[i]}) shape: {fm.shape}")

            # Expected shapes for width_multiplier=1.0 and indices (2, 3, 4)
            # Input: [1, 128, 64, 64] (Assumed input to backbone)
            # MobileViTv2 internal processing starts from its conv_1 expecting 3 channels.
            # THIS IS A PROBLEM: The wrapper needs to adapt the input channels.
            # Let's rethink: The wrapper should replace the *entire* backbone section
            # starting from the raw image input, or have an initial stem matching YOLO.

            # --- REVISED APPROACH ---
            # The backbone should likely take the raw image input (B, 3, H, W)
            print("\n--- Revising Test: Assuming backbone takes image input ---")
            backbone_img_input = MobileViTv2Backbone(width_multiplier=1.0)
            dummy_img_input = torch.randn(1, 3, 256, 256) # Standard image input
            print(f"Input shape: {dummy_img_input.shape}")
            output_features_img = backbone_img_input(dummy_img_input)
            print(f"Obtained {len(output_features_img)} feature maps:")
            # Expected shapes for indices (2, 3, 4) with 256x256 input:
            # Index 2 (Layer 2 output): [1, 128, 64, 64] (P3/8)
            # Index 3 (Layer 3 output): [1, 256, 32, 32] (P4/16)
            # Index 4 (Layer 4 output): [1, 384, 16, 16] (P5/32)
            for i, fm in enumerate(output_features_img):
                print(f"  Feature map {i} (from index {backbone_img_input.return_indices[i]}) shape: {fm.shape}")
            print(f"Output channels property: {backbone_img_input.out_channels}") # Should be [128, 256, 384]

        except Exception as e:
            print(f"Error during testing: {e}")
            import traceback
            traceback.print_exc()

    else:
        print("Skipping MobileViTv2Backbone test because cvnets is not available.")

```

**Explanation and Key Points:**

1.  **File Location:** Save this code as `mobilevit.py` inside modules.
2.  **`cvnets` Dependency:** It explicitly tries to import `MobileViTv2` and `get_configuration` from `cvnets`. If `cvnets` is not installed or has issues, it prints a warning and raises an error if you try to instantiate the class.
3.  **`__init__` Method:**
    *   Takes `width_multiplier` and `return_indices` as arguments.
    *   Sets up the `opts` namespace object, replicating the configuration needed for `MobileViTv2`, similar to your `demorun.py`. `width_multiplier` is passed through. Dropout is set to 0.0, which is common when using a model as a feature backbone.
    *   Instantiates `self.mobilevit = MobileViTv2(opts=opts)`.
    *   **Crucially**, it uses `get_configuration(opts)` to determine the channel sizes of all intermediate layers produced by MobileViTv2 based on the `width_multiplier`.
    *   It then selects the channel counts corresponding to the `return_indices` and stores them in `self._out_channels`.
    *   Includes a placeholder comment for loading `pretrained` weights (this would require specific code depending on where the weights come from).
    *   Removes the final classifier from the loaded `mobilevit` model.
4.  **`forward` Method:**
    *   Takes the input tensor `x`.
    *   Calls the `self.mobilevit(x)` forward method. Since you modified the `MobileViTv2` forward method in your provided file to return a list of feature maps, this wrapper uses that directly.
    *   It selects the specific feature maps from the returned list using the `self.return_indices`.
    *   Returns the list of selected feature maps.
5.  **`out_channels` Property:** This property allows the Ultralytics `parse_model` function to query the number of output channels for each feature map returned by this module. This is essential for connecting subsequent layers correctly in the YAML file.
6.  **Example Usage (`if __name__ == '__main__':`)**
    *   Includes a basic test block.
    *   **Important Correction:** My initial test assumed the backbone would take input from a previous YOLO layer. However, it's more standard for a replacement backbone to take the *original image input* (e.g., `[B, 3, H, W]`). The revised test reflects this, showing the expected output shapes for P3, P4, and P5 scales when feeding a `[1, 3, 256, 256]` image. The output channels should be `[128, 256, 384]` for `width_multiplier=1.0`.

Now you have the wrapper class. The next steps are:
1.  Modify tasks.py (`parse_model` function) to recognize and use this new module.
2.  Update your `yolo11.yaml` file to use `MobileViTv2Backbone` correctly.