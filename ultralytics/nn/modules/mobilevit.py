import os
import sys
import torch
import torch.nn as nn
import argparse
from typing import List, Tuple

root = os.path.abspath(os.path.join(__file__, "../../../../"))
sys.path.insert(0, root)

# --- Attempt to import MobileViTv2 from cvnets ---
# Ensure the cvnets library is installed and accessible in your environment
# try:
from cvnets.cvnets.models.classification.mobilevit_v2 import MobileViTv2
# We also need the config function to determine output channels dynamically
from cvnets.cvnets.models.classification.config.mobilevit_v2 import get_configuration
CVNETS_AVAILABLE = True
# except ImportError:
#     print("Warning: cvnets library not found or MobileViTv2 could not be imported. "
#           "MobileViTv2Backbone module will not be functional.")
#     MobileViTv2 = None # Define as None to allow conditional checks
#     get_configuration = None
#     CVNETS_AVAILABLE = False
# # --- End Import ---

class MobileViTv2Backbone(nn.Module):
    """
    Wrapper module for using MobileViTv2 (from cvnets) as a backbone feature extractor
    in the Ultralytics framework.
    """
    def __init__(self, width_multiplier: float = 1.0, return_indices: Tuple[int] = (2, 3, 4), pretrained: bool = False):
        """
        Initializes the MobileViTv2Backbone wrapper.

        Args:
            width_multiplier (float): Width multiplier for MobileViTv2 config.
            return_indices (Tuple[int]): Indices of the feature maps to return from MobileViTv2's output list.
                                         Defaults to (2, 3, 4) corresponding roughly to P3, P4, P5 stages.
            pretrained (bool): If True, attempts to load pretrained weights (Not implemented in this basic wrapper).
        """
        super().__init__()

        if not CVNETS_AVAILABLE:
            raise ImportError("cvnets library is required to use MobileViTv2Backbone, but it's not installed or accessible.")

        self.return_indices = return_indices

        # --- Configure MobileViTv2 using opts ---
        # Create a namespace to mimic the argument parsing used by cvnets
        opts = argparse.Namespace()

        # Set essential MobileViTv2 configuration options
        setattr(opts, "model.classification.mitv2.width_multiplier", width_multiplier)
        setattr(opts, "model.classification.n_classes", 1000) # Placeholder, not used for feature extraction
        setattr(opts, "model.layer.global_pool", "mean") # Placeholder, not used for feature extraction
        setattr(opts, "model.classification.mitv2.dropout", 0.0) # Typically disable dropout for backbone features
        setattr(opts, "model.classification.mitv2.attn_dropout", 0.0)
        setattr(opts, "model.classification.mitv2.ffn_dropout", 0.0)
        setattr(opts, "model.classification.mitv2.attn_norm_layer", "layer_norm_2d")
        setattr(opts, "common.enable_coreml_compatible_module", False)
        setattr(opts, "model.classification.enable_layer_wise_lr_decay", False) # Not relevant for backbone use
        setattr(opts, "model.classification.layer_wise_lr_decay_rate", 0.9) # Not relevant
        setattr(opts, "model.normalization.name", "batch_norm") # Standard norm
        setattr(opts, "model.normalization.momentum", 0.1)
        setattr(opts, "model.normalization.groups", 1)
        setattr(opts, "model.activation.name", "swish") # Common activation
        setattr(opts, "model.activation.inplace", False)
        setattr(opts, "model.activation.neg_slope", 0.1) # For leaky relu if used, but swish is default

        # Add any other options required by the specific MobileViTv2 version if needed
        # --- End opts setup ---

        # Instantiate the core MobileViTv2 model
        # Note: MobileViTv2 internally removes the final classifier if num_classes=0,
        # but we are modifying its forward pass anyway.
        self.mobilevit = MobileViTv2(opts=opts)

        # --- Determine output channels ---
        # We need the configuration to know the channel dimensions at each stage
        mobilevit_config = get_configuration(opts=opts)
        # Store channel dimensions of all layers generated by MobileViTv2
        # Order: conv_1, layer_1, layer_2, layer_3, layer_4, layer_5, conv_1x1_exp
        all_layer_channels = [
            mobilevit_config["layer0"]["out_channels"],
            mobilevit_config["layer1"]["out_channels"],
            mobilevit_config["layer2"]["out_channels"],
            mobilevit_config["layer3"]["out_channels"],
            mobilevit_config["layer4"]["out_channels"],
            mobilevit_config["layer5"]["out_channels"],
            mobilevit_config["layer5"]["out_channels"], # conv_1x1_exp is Identity, keeps channels from layer5
        ]
        # Select the channels corresponding to the features we will return
        self._out_channels = [all_layer_channels[i] for i in self.return_indices]

        # --- Pretrained weights ---
        if pretrained:
            # TODO: Implement loading pretrained weights if available/needed
            # This would typically involve finding a weight file and loading
            # the state_dict into self.mobilevit, potentially mapping keys.
            print("Warning: Pretrained weights loading not implemented in this wrapper.")
            pass

        # Remove the original classifier explicitly if it exists (MobileViTv2 might already handle this)
        # hasattr check is safer
        if hasattr(self.mobilevit, 'classifier'):
            del self.mobilevit.classifier
            self.mobilevit.classifier = nn.Identity() # Replace with identity


    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        """
        Runs the forward pass through MobileViTv2 and returns selected feature maps.

        Args:
            x (torch.Tensor): Input tensor (typically from the layer before the backbone in YOLO).

        Returns:
            List[torch.Tensor]: A list of the selected feature map tensors.
        """
        # MobileViTv2's forward method (as modified in the provided file)
        # already returns a list of all intermediate feature maps.
        all_feature_maps = self.mobilevit(x)

        # Select only the feature maps specified by self.return_indices
        selected_features = [all_feature_maps[i] for i in self.return_indices]

        return selected_features

    @property
    def out_channels(self) -> List[int]:
        """
        Returns the number of output channels for each feature map returned by the forward pass.
        Needed by the Ultralytics model parser.
        """
        return self._out_channels

# filepath: /home/mrmfr/afterMidpointZaka/TASK3-YOLO-ViT-Backbone/ultralytics/nn/modules/mobilevit.py
# ... existing code ...

# Example Usage (for testing within this file)
if __name__ == '__main__':
    if CVNETS_AVAILABLE:
        # Test with default settings (width_multiplier=1.0)
        print("Testing MobileViTv2Backbone (width_multiplier=1.0)...")

        # --- Test: Assuming backbone takes image input ---
        backbone_img_input = MobileViTv2Backbone(width_multiplier=1.0)
        dummy_img_input = torch.randn(1, 3, 256, 256) # Standard image input
        print(f"Input shape: {dummy_img_input.shape}")

        try:
            output_features_img = backbone_img_input(dummy_img_input)
            print(f"Obtained {len(output_features_img)} feature maps:")
            # Expected shapes for indices (2, 3, 4) with 256x256 input:
            # Index 2 (Layer 2 output): [1, 128, 32, 32] (P3/8) -> Corrected stride
            # Index 3 (Layer 3 output): [1, 256, 16, 16] (P4/16) -> Corrected stride
            # Index 4 (Layer 4 output): [1, 384, 8, 8]  (P5/32) -> Corrected stride
            # Note: The exact output shapes depend on strides in MobileViTv2 config.
            # Let's verify based on the config and forward pass:
            # conv_1: stride 2 -> H/2, W/2
            # layer_1: stride 1 -> H/2, W/2
            # layer_2: stride 2 -> H/4, W/4
            # layer_3: stride 2 -> H/8, W/8  (Index 2 output) -> [1, 128, 32, 32] for 256x256 input
            # layer_4: stride 2 -> H/16, W/16 (Index 3 output) -> [1, 256, 16, 16] for 256x256 input
            # layer_5: stride 2 -> H/32, W/32 (Index 4 output) -> [1, 384, 8, 8] for 256x256 input
            for i, fm in enumerate(output_features_img):
                print(f"  Feature map {i} (from index {backbone_img_input.return_indices[i]}) shape: {fm.shape}")
            print(f"Output channels property: {backbone_img_input.out_channels}") # Should be [128, 256, 384] for width=1.0

        except Exception as e:
            print(f"Error during testing: {e}")
            import traceback
            traceback.print_exc()

    else:
        print("Skipping MobileViTv2Backbone test because cvnets is not available.")

# ... rest of file if any ...